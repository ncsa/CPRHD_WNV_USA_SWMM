{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMM Hourly and Daily Precipitation Comparison\n",
    "\n",
    "In this notebook we will download the 3-hour and daily precipitation datasets from NARR, and then run simulations on both datasets. We will then compare their outputs.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Usage\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wget  # Downloading Data\n",
    "\n",
    "# Raster Data\n",
    "from osgeo import gdal  # GDAL\n",
    "import rasterio as rio  # Modifying Raster datasets\n",
    "import rasterstats as rs\n",
    "import geopandas as gpd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "# Multi-processing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    " \n",
    "    \n",
    "# gdal_calc.py, custom path\n",
    "path_to_gdal_calc = '/home/matas/anaconda3/envs/swmm/bin/'\n",
    "sys.path.insert(0, path_to_gdal_calc)\n",
    "try:\n",
    "    import gdal_calc\n",
    "except:\n",
    "    print('gdal_calc not found, please specify the path to this file in gdal_path above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Creating the Land Mask\n",
    "\n",
    "This land mask will be used to remove the ocean data in both the daily and hourly data-sets.\n",
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./land.nc'):\n",
    "    print('File not found, attempting download.')\n",
    "    download_url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/time_invariant/land.nc'\n",
    "    wget.download(download_url, './land.nc')\n",
    "    \n",
    "mask_file = glob.glob('./land.nc')\n",
    "print('Successfully found land mask' if len(mask_file)==1 else 'Failed to find land mask')\n",
    "mask_file = mask_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection\n",
    "\n",
    "We use the EPSG 5070 projection, which matches our block group shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "subdataset = 'land'\n",
    "output_file = './land_mask'\n",
    "\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + mask_file + ':' + subdataset, destNameOrDestDS=output_file + '.geotiff', options=warp_options)\n",
    "ds = None  # THIS IS VERY IMPORTANT\n",
    "print('Successfully Warped Land Mask (' + output_file + '.geotiff)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Land Mask Extension\n",
    "\n",
    "We will extend the land mask by one pixel in each direction if the target pixel has no data. This is because the land mask excludes some block groups due to the low resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = rio.open(output_file + '.geotiff')\n",
    "\n",
    "data = raster.read(1)\n",
    "\n",
    "rows = data.shape[0]\n",
    "cols = data.shape[1]\n",
    "nodata = 0\n",
    "\n",
    "data = np.ma.masked_equal(data,nodata)\n",
    "output_data = np.copy(data)\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if data.mask[i][j]:\n",
    "            try:\n",
    "                for x in range(-1,2):\n",
    "                    for y in range(-1,2):\n",
    "                        if not data.mask[i+x][j+y]:\n",
    "                            output_data[i][j] = data[i+x][j+y]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "with rio.open(output_file + '_extended.geotiff', 'w', **raster.profile) as dst:\n",
    "    dst.write(output_data, 1)    \n",
    "    print('Created extended land mask at', output_file + '_extended.geotiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data\n",
    "\n",
    "## 1) Downloading the 3-Hour NetCDF Data for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './hourly/'\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "name = 'apcp.2014.nc'\n",
    "url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/monolevel/'\n",
    "\n",
    "\n",
    "if not os.path.exists(download_dir + name) or overwrite:\n",
    "    print('Downloading', download_dir + name)\n",
    "    wget.download(url + name, download_dir + name)\n",
    "else:\n",
    "    print('File already exists, not downloading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Converting NetCDF to GeoTIFF\n",
    "\n",
    "**Note that this process will take a few minutes as the NETCDF has a lot of bands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf = glob.glob('./hourly/apcp.2014.nc')\n",
    "print('Found NETCDF file' if netcdf else 'NETCDF File not found')\n",
    "if not netcdf:\n",
    "    raise KeyboardInterrupt\n",
    "netcdf = netcdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False  # Overwrite existing GeoTIFF or not\n",
    "\n",
    "proj = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "\n",
    "# The warp options specify our target projection and output type (GeoTIFF)\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "subdataset = 'apcp'\n",
    "\n",
    "outfile = './hourly/apcp.hourly.2014.geotiff'\n",
    "\n",
    "if not os.path.exists(outfile) or overwrite:\n",
    "    ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + netcdf + ':' + subdataset, destNameOrDestDS=outfile, options=warp_options)\n",
    "    ds = None # Flush the file cache\n",
    "    print('Warped file ./hourly/apcp.hourly.2014.geotiff')\n",
    "else:\n",
    "    print('File already exists, skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Masking the Data\n",
    "\n",
    "#### Select the NetCDF and Land Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf = glob.glob('./hourly/apcp.hourly.2014.geotiff')\n",
    "print('Found NETCDF file' if netcdf else 'NETCDF File not found')\n",
    "\n",
    "mask_file = glob.glob('./land_mask_extended.geotiff')\n",
    "print('Found Mask file' if mask_file else 'Mask file not found')\n",
    "\n",
    "if not mask_file or not netcdf:\n",
    "    print('Error! One or more required files not found.')\n",
    "    raise KeyboardInterrupt\n",
    "    \n",
    "netcdf = netcdf[0]\n",
    "mask_file = mask_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask Each Band\n",
    "\n",
    "Note that this will take a while as there are 2920 bands!\n",
    "\n",
    "We will use multiprocessing to speed up the masking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "output_directory = './hourly/masked_geotiffs/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_mask(band, netcdf, mask):\n",
    "    overwrite = False\n",
    "    outfile = output_directory + str(band) + '.geotiff'\n",
    "    if not os.path.exists(outfile) or overwrite:\n",
    "        gdal_calc.Calc('A*B', A=netcdf, B=mask, A_band=band, outfile=outfile, format='GTiff', NoDataValue=-9999.0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = gdal.Open(netcdf)\n",
    "band_count = raster.RasterCount\n",
    "\n",
    "band_list = [i for i in range(1, band_count+1)]\n",
    "\n",
    "pool = Pool()\n",
    "pool.map(partial(multithread_mask, netcdf=netcdf, mask=mask_file), band_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Extracting the Data\n",
    "\n",
    "The shape file california_chicago.gpkg is a modified version of the SelectBG_all_land_BGID_final shapefile and contains only two block groups. It can be found on the [GitHub repository](https://github.com/ncsa/CPRHD_WNV_USA_SWMM/tree/master/jupyter_notebooks/SWMM_Precipitation) in this Jupyter Notebook's folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_file = './chicago_california_shape_file/california_chicago.gpkg'\n",
    "geotiffs = glob.glob('./hourly/masked_geotiffs/*.geotiff')\n",
    "if not shape_file and len(geotiffs) != 2920:\n",
    "    print('Failed to find one or more files!')\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    print('Found all files successfully')\n",
    "    \n",
    "geotiffs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "shape_file = './modified_shapefile/california_chicago.gpkg'\n",
    "shape_frame = gpd.read_file(shape_file)\n",
    "shape_frame['GEOID10'] = shape_frame['GEOID10'].astype(str)\n",
    "name = (geotiffs[0])[geotiffs[0].rfind('/')+1:geotiffs[0].rfind('.')]\n",
    "\n",
    "output_columns = ['mean']\n",
    "stats = rs.zonal_stats(shape_frame, geotiffs[0], stats=output_columns, all_touched=True)\n",
    "frame = pd.DataFrame.from_dict(stats)\n",
    "frame = frame.join(shape_frame['GEOID10'])\n",
    "frame = frame.join(shape_frame['STATE'])\n",
    "frame = frame[['STATE', 'GEOID10', 'mean']]\n",
    "\n",
    "columns = frame.columns.values\n",
    "columns[2] = name\n",
    "frame.columns = columns\n",
    "\n",
    "for file in geotiffs[1:]:\n",
    "    name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    stats = rs.zonal_stats(shape_frame, file, stats=output_columns, all_touched=True)\n",
    "    subframe = pd.DataFrame.from_dict(stats)\n",
    "    \n",
    "    columns = subframe.columns.values\n",
    "    columns[0] = name\n",
    "    subframe.columns = columns\n",
    "    frame = frame.join(subframe)\n",
    "    \n",
    "frame.to_pickle('./hourly/hourly_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Unit Conversion and Sorting\n",
    "\n",
    "We will convert NARR's units (mm) to the units we use in SWMM (inches).\n",
    "\n",
    "We will also sort the columns by increasing order (1, 2, 3, ..., 2919, 2920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_pickle('./hourly/hourly_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = frame.iloc[0,:]\n",
    "chicago = frame.iloc[1,:]\n",
    "\n",
    "# Convert to inches and append to the STATE and GEOID10 rows\n",
    "chicago = chicago.iloc[2:].apply(lambda x: x / 25.40)\n",
    "california = california.iloc[2:].apply(lambda x: x / 25.40)\n",
    "\n",
    "#Sort by increasing order\n",
    "chicago.index = chicago.index.astype(int)\n",
    "chicago.sort_index(inplace=True)\n",
    "chicago = chicago.rename('chicago')\n",
    "\n",
    "california.index = california.index.astype(int)\n",
    "california.sort_index(inplace=True)\n",
    "california = california.rename('california')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './hourly/precipitation_data/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "chicago.to_pickle(output_directory + 'chicago.pkl')\n",
    "california.to_pickle(output_directory + 'california.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Data Processing\n",
    "\n",
    "We will repeat the above process with the daily NetCDF for 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './daily/'\n",
    "if not os.path.exists(download_dir):\n",
    "    print('Making', download_dir)\n",
    "    os.mkdir(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/Dailies/monolevel/'\n",
    "name = 'apcp.2014.nc'\n",
    "\n",
    "if not os.path.exists(download_dir + name):\n",
    "    print('NetCDF file not found, attempting download')\n",
    "    wget.download(download_url+name, download_dir + name)\n",
    "    print('Download completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Convert the NetCDF to GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf = glob.glob('./daily/apcp.2014.nc')\n",
    "print('Found NETCDF file' if netcdf else 'NETCDF File not found')\n",
    "if not netcdf:\n",
    "    raise KeyboardInterrupt\n",
    "netcdf = netcdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False  # Overwrite existing GeoTIFF or not\n",
    "\n",
    "proj = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "\n",
    "# The warp options specify our target projection and output type (GeoTIFF)\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "subdataset = 'apcp'\n",
    "if not os.path.exists('./daily/apcp.daily.2014.geotiff') or overwrite:\n",
    "    ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + netcdf + ':' + subdataset, destNameOrDestDS='./daily/apcp.daily.2014.geotiff', options=warp_options)\n",
    "    ds = None # Flush the file cache\n",
    "    print('Warped file ./daily/apcp.daliy.2014.geotiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Masking each GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf = glob.glob('./apcp.daily.2014.geotiff')\n",
    "print('Found NETCDF file' if netcdf else 'NETCDF File not found')\n",
    "if not netcdf:\n",
    "    raise KeyboardInterrupt\n",
    "netcdf = netcdf[0]\n",
    "\n",
    "mask_file = glob.glob('./land_mask_extended.geotiff')\n",
    "print('Found mask file' if mask_file else 'Mask file not found')\n",
    "if not mask_file:\n",
    "    raise KeyboardInterrupt\n",
    "mask_file = mask_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "output_directory = './daily/masked_geotiffs/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)\n",
    "    \n",
    "raster = gdal.Open(netcdf)\n",
    "band_count = raster.RasterCount\n",
    "print(netcdf)\n",
    "for band in range(1, band_count+1):\n",
    "    outfile = output_directory + str(band) + '.geotiff'\n",
    "    if not os.path.exists(outfile) or overwrite:\n",
    "        gdal_calc.Calc('A*B', A=netcdf, B=mask_file, A_band=band, outfile=outfile, format='GTiff', NoDataValue=-9999.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Zonal Stats Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiffs = glob.glob('./daily_masked_geotiffs/*.geotiff')\n",
    "if len(geotiffs) != 365:\n",
    "    print('Failed to find all GeoTIFFs.')\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    print('Found all GeoTIFF files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_file = './modified_shapefile/california_chicago.gpkg'\n",
    "if not os.path.exists(shape_file):\n",
    "    print('Failed to find shape file')\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    print('Found shape file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './daily/precipitation_data/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_frame = gpd.read_file(shape_file)\n",
    "shape_frame['GEOID10'] = shape_frame['GEOID10'].astype(str)\n",
    "\n",
    "# Set up the frame with the first file (we will join the other files onto this one)\n",
    "name = (geotiffs[0])[geotiffs[0].rfind('/')+1:geotiffs[0].rfind('.')]\n",
    "\n",
    "output_columns = ['mean']\n",
    "stats = rs.zonal_stats(shape_frame, geotiffs[0], stats=output_columns, all_touched=True)\n",
    "frame = pd.DataFrame.from_dict(stats)\n",
    "frame = frame.join(shape_frame['GEOID10'])\n",
    "frame = frame.join(shape_frame['STATE'])\n",
    "frame = frame[['STATE', 'GEOID10', 'mean']]\n",
    "\n",
    "columns = frame.columns.values\n",
    "columns[2] = name\n",
    "frame.columns = columns\n",
    "\n",
    "for file in geotiffs[1:]:\n",
    "    name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    stats = rs.zonal_stats(shape_frame, file, stats=output_columns, all_touched=True)\n",
    "    subframe = pd.DataFrame.from_dict(stats)\n",
    "    \n",
    "    columns = subframe.columns.values\n",
    "    columns[0] = name\n",
    "    subframe.columns = columns\n",
    "    frame = frame.join(subframe)\n",
    "frame.to_pickle('./daily/daily_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Unit Conversion and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = pd.read_pickle('./daily/daily_data.pkl')\n",
    "\n",
    "california = daily_data.iloc[0,:]\n",
    "chicago = daily_data.iloc[1,:]\n",
    "\n",
    "# Convert to inches and append to the STATE and GEOID10 rows\n",
    "chicago = chicago.iloc[2:].apply(lambda x: x / 25.40)\n",
    "california = california.iloc[2:].apply(lambda x: x / 25.40)\n",
    "\n",
    "#Sort by increasing order\n",
    "chicago.index = chicago.index.astype(int)\n",
    "chicago.sort_index(inplace=True)\n",
    "chicago = chicago.rename('chicago')\n",
    "\n",
    "california.index = california.index.astype(int)\n",
    "california.sort_index(inplace=True)\n",
    "california = california.rename('california')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './daily/precipitation_data/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "chicago.to_pickle(output_directory + 'chicago.pkl')\n",
    "california.to_pickle(output_directory + 'california.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been processed, we can analyze it and use it to run simulations. This can be found in the Data_Analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
