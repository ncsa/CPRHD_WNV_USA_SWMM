{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Evaporation Data for each Year\n",
    "We will obtain the evaporation data for each month for each year across each blockgroup. This will then be implemented in the input file as a Timeseries\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evap.mon.mean.nc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/Monthlies/monolevel/evap.mon.mean.nc'\n",
    "\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert NetCDF to GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_5070 = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "warp_options = gdal.WarpOptions(options = '-t_srs \\\"' + projection_5070 + '\\\" -of GTiff')\n",
    "\n",
    "ds = gdal.Warp('./evap.mon.mean.geotiff', 'NETCDF:./evap.mon.mean.nc:evap', options=warp_options)\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Bands of the GeoTIFF\n",
    "\n",
    "The NetCDF data starts recording from January 1979 (Band 1). We only want the data from January 1981 to December 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './geotiffs/geotiffs/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 25\n",
      "End: 432\n"
     ]
    }
   ],
   "source": [
    "start_year = 1981\n",
    "start_month = 1\n",
    "end_year = 2014\n",
    "end_month = 12\n",
    "\n",
    "start_band = (start_year - 1979) * 12 + start_month\n",
    "end_band = (end_year - 1979) * 12 + end_month\n",
    "print('Start:', start_band)\n",
    "print('End:', end_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:25<00:00, 16.30it/s]\n"
     ]
    }
   ],
   "source": [
    "in_ds = gdal.Open('./evap.mon.mean.geotiff')\n",
    "\n",
    "for band in tqdm(range(start_band, end_band+1)):\n",
    "    outfile = out_dir + str(band) + '.geotiff'\n",
    "    \n",
    "    translate_options = gdal.TranslateOptions(options='-b ' + str(band) + ' -of GTiff')\n",
    "    gdal.Translate(outfile, in_ds, options=translate_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some block groups lie in the ocean, this will cause incorrect evaporation data for them as the oceans have higher evaporation rates.\n",
    "\n",
    "<img src=\"https://i.imgur.com/F6UIyls.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking the GeoTIFFs\n",
    "To solve this issue, we will mask the evaporation data using the land mask provided by NARR NCEP.\n",
    "\n",
    "##### Download the Land Mask File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'land.nc'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/time_invariant/land.nc'\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Project the Land Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_5070 = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "warp_options = gdal.WarpOptions(options = '-t_srs \\\"' + projection_5070 + '\\\" -of GTiff')\n",
    "\n",
    "ds = gdal.Warp('./land.geotiff', 'NETCDF:./land.nc:land', options=warp_options)\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Gdal_calc.py\n",
    "This will be used to mask the files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal_path = '/home/matas/anaconda3/envs/swmm/bin/'\n",
    "sys.path.insert(0, gdal_path)\n",
    "import gdal_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './geotiffs/masked_geotiffs/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_mask = './land.geotiff'\n",
    "\n",
    "for geotiff in glob.glob('./geotiffs/geotiffs/*.geotiff'):\n",
    "    print(band_number)\n",
    "    band_number = geotiff[geotiff.rfind('/')+1:geotiff.rfind('.')]\n",
    "    outfile = out_dir + band_number + '.geotiff'\n",
    "    \n",
    "    gdal_calc.Calc('A*B', A=geotiff, B=land_mask, outfile=outfile, format='GTiff', NoDataValue=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the block groups that were affected have NoData, since we specified NoData = 0.\n",
    "<img src=\"https://i.imgur.com/povTQzI.png\">\n",
    "\n",
    "We will interpolate this missing data from data values around the missing data pixels, extending the raster by one pixel in each direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Interpolation\n",
    "\n",
    "We will interpolate the data in a 3x3 grid around the target pixel. We will only modify a pixel if it has no data, and if it has at least one neighbor with data.\n",
    "\n",
    "Neighbors directly adjacent to the target pixel (North, East, South, West) will receive a weight of 1.\n",
    "\n",
    "Neighbors diagonally adjacent to the target pixel (NW, NE, SE, SW) will receive a weight of $\\dfrac{1}{\\sqrt{2}}$, since they are farther. Note that each pixel in this dataset represents about 32km$^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './geotiffs/masked_extended_geotiffs/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import numpy as np\n",
    "\n",
    "for geotiff in glob.glob('./geotiffs/masked_geotiffs/*.geotiff'):\n",
    "    name = geotiff[geotiff.rfind('/')+1:geotiff.rfind('.')]\n",
    "    print(name)\n",
    "    raster = rio.open(geotiff)\n",
    "    data = raster.read(1)\n",
    "    \n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    nodata = raster.nodatavals\n",
    "    data = np.ma.masked_equal(data, nodata)\n",
    "    output = np.copy(data)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if data.mask[i][j]:\n",
    "                try:\n",
    "                    total = 0\n",
    "                    count = 0\n",
    "                    for x in range(-1,2):\n",
    "                        for y in range(-1,2):\n",
    "                            if not data.mask[i+x][j+y]:\n",
    "                                if (x == -1 or x == 1) and (y == -1 or y == 1):\n",
    "                                    total += data[i+x][j+y] / 1.41421356237\n",
    "                                else:\n",
    "                                    total += data[i+x][j+y]\n",
    "                                count+= 1\n",
    "                                \n",
    "                    output[i][j] = total / count\n",
    "                except: # Array index error\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "    with rio.open(out_dir + name + '.geotiff', 'w', **raster.profile) as dst:\n",
    "        dst.write(output, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation Result\n",
    "<img src=\"https://i.imgur.com/gZSiuIs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zonal Stats\n",
    "We will now load the block group shapefile and use rasterstats' zonal_stats function to calculate the evaporation data for each block group.\n",
    "\n",
    "Note that this will take a long time for each file because there are ~150,000 block groups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterstats as rs\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import persistqueue\n",
    "\n",
    "shape_file = '../../../data/input_file_data/SelectBG_all_land_BGID_final.gpkg'\n",
    "shape_frame = gpd.read_file(shape_file)\n",
    "shape_frame['GEOID10'] = shape_frame['GEOID10'].astype(str)\n",
    "output_stats = ['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making ./evap/\n"
     ]
    }
   ],
   "source": [
    "out_dir = './evap/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistent Queue\n",
    "\n",
    "This is a disk-based queue which will save progress in case of a program crash. The files are inserted using q.put() and retrieved with q.get(). \n",
    "\n",
    "Once the file is processed we can remove it from the queue using q.ack(). If the program were to crash between q.get() and q.ack(), the file would return to the queue and be processed during a later run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = persistqueue.UniqueAckQ('./queue/', auto_commit=True, multithreading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker():\n",
    "    while True:\n",
    "        file = q.get(block=True)\n",
    "        band = file[file.rfind('/')+1:file.rfind('.')]\n",
    "        print('Started', band)\n",
    "        stats = rs.zonal_stats(shape_frame, file, stats=output_stats, all_touched=True)\n",
    "        frame = pd.DataFrame.from_dict(stats)\n",
    "        frame = frame.join(shape_frame['GEOID10'])\n",
    "        frame.to_pickle(out_dir + band + '.pkl')\n",
    "        print('Finished', band)\n",
    "        q.ack(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-processing\n",
    "We will create 8 worker processes, you can change this number if your computer has more CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if q.size == 0:\n",
    "    print('Queue is empty, adding files.') \n",
    "    for file in glob.glob('./masked_extended_geotiffs/*.geotiff'):\n",
    "        q.put(file)\n",
    "\n",
    "for i in range(8):\n",
    "    p = Process(target=worker)\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting the Output, Converting Units\n",
    "\n",
    "We will load each month's pickle file and format them by month and year.\n",
    "\n",
    "Also, we will convert from millimeters to inches.\n",
    "\n",
    "##### Function to Convert Bands to Datetime Index\n",
    "\n",
    "This function will convert the 1-indexed NARR bands (1 = January 1979, 2 = February 1979, etc.) to a datetime index which will be used as our columns in the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import datetime\n",
    "def narr_band_to_datetime(band_number):\n",
    "    month = band_number % 12\n",
    "    if month == 0:\n",
    "        month = 12\n",
    "    year = 1979 + (band_number-1) // 12\n",
    "    return datetime.datetime(year, month, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./evap/*.pkl')\n",
    "\n",
    "# Create the initial dataframe using the first file in the list\n",
    "name = files[0][files[0].rfind('/')+1:files[0].rfind('.')]\n",
    "frame = pd.read_pickle(files[0])\n",
    "frame.set_index('GEOID10', drop=True, inplace=True)\n",
    "frame.rename(columns={'mean':narr_band_to_datetime(int(name))}, inplace=True)\n",
    "\n",
    "\n",
    "# Add onto the initial dataframe using the rest of the files\n",
    "for file in files[1:]:\n",
    "    name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    subframe = pd.read_pickle(file)\n",
    "    subframe.set_index('GEOID10', drop=True, inplace=True)\n",
    "    subframe.rename(columns={'mean':narr_band_to_datetime(int(name))}, inplace=True)\n",
    "    frame = frame.join(subframe)\n",
    "    \n",
    "    \n",
    "print(frame)\n",
    "frame.to_pickle('./evaporation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the Columns, Convert to Inches\n",
    "\n",
    "The conversion for inches is as follows:\n",
    "\n",
    "inches / day (SWMM) = mm(NARR) * 8 * 0.0393701\n",
    "\n",
    "We multiply by 8 because NARR accumulates on a 3-hourly interval (8 per day).\n",
    "\n",
    "1 millimeter = 0.0393701 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_pickle('./evaporation.pkl')\n",
    "frame = frame.sort_index(axis='columns')  # Sort by Year and Month\n",
    "frame = frame.apply(lambda x : x * 8 * 0.0393701)  # Convert to inches\n",
    "frame.to_pickle('./evaporation_converted.pkl')\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare New Data to Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Data:\n",
      "1     0.045161\n",
      "2     0.068187\n",
      "3     0.106026\n",
      "4     0.157164\n",
      "5     0.185812\n",
      "6     0.193588\n",
      "7     0.176125\n",
      "8     0.150855\n",
      "9     0.125562\n",
      "10    0.088352\n",
      "11    0.054638\n",
      "12    0.041084\n",
      "Name: 010010202002, dtype: float64\n",
      "Old Data:\n",
      "1981-01-01    0.036854\n",
      "1981-02-01    0.064368\n",
      "1981-03-01    0.104568\n",
      "1981-04-01    0.169002\n",
      "1981-05-01    0.172171\n",
      "                ...   \n",
      "2014-08-01    0.164870\n",
      "2014-09-01    0.118315\n",
      "2014-10-01    0.090916\n",
      "2014-11-01    0.044849\n",
      "2014-12-01    0.037901\n",
      "Name: 010010202002, Length: 408, dtype: float64\n",
      "Mean Difference: 0.003139681828711702\n",
      "Max Difference: 0.04936140216635991\n",
      "Min Difference: -0.06297576178434318\n"
     ]
    }
   ],
   "source": [
    "new_frame = pd.read_pickle('./evaporation_converted.pkl')\n",
    "new_data = new_frame.loc['010010202002']\n",
    "new_data = new_data.groupby(new_data.index.month).mean()\n",
    "\n",
    "old_frame = pd.read_pickle('../../../data/input_file_data/evaporation_converted.pkl')\n",
    "old_data = old_frame.loc['010010202002']\n",
    "\n",
    "print('New Data:\\n' + str(new_data))\n",
    "print('Old Data:\\n' + str(old_data))\n",
    "\n",
    "old_data = old_data.reset_index(drop=True)\n",
    "print('Mean Difference:', str((new_data - old_data).mean()))\n",
    "print('Max Difference:', str((new_data - old_data).max()))\n",
    "print('Min Difference:', str((new_data - old_data).min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We see that the mean difference between the old and new data is very small. We have successfully obtained monthly evaporation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
