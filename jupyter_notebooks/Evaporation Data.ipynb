{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMM Green Infrastructure Project Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NARR Evaporation Data Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how the evaporation data was extracted from a NetCDF file.\n",
    "\n",
    "All source code can be found on [the Github repository](https://github.com/ncsa/CPRHD_WNV_USA_SWMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Preparation\n",
    "\n",
    "Although this data extraction relies heavily on Python, we also use QGIS in this process. \n",
    "Therefore, it is required to [download QGIS](https://www.qgis.org/en/site/forusers/download.html). At the time of writing, the latest version is 3.8.\n",
    "\n",
    "QGIS is commonly used in GIS to perform calculations on rasters and vectors. We will use it to calculate the average evaporation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Acquiring the Data\n",
    "\n",
    "#### Evaporation Data\n",
    "We obtained the evaporation data in [NetCDF format from NOAA](https://www.esrl.noaa.gov/psd/data/gridded/data.narr.monolevel.html). NetCDF is a file format that is used to store multi-dimensional data such as temperature, humidity, and in our case, evaporation.\n",
    "\n",
    "We used the [evap.mon.mean file](ftp://ftp.cdc.noaa.gov/Datasets/NARR/Monthlies/monolevel/evap.mon.mean.nc), which represents monthly mean accumulated total evaporation. At the time of writing, NARR has provided data from 1979/01/01 to 2019/10/30. Each month is stored in a separate band of the NetCDF file, leading to 489 bands.\n",
    "\n",
    "We only need the data from 01/1979 to 12/2014 for this project, which corresponds to bands 1 to 432.\n",
    "\n",
    "Additionally, each NetCDF file contains subdatasets, typically this includes time, latitude, and longitude bands, in addition to the data we are interested in.\n",
    "\n",
    "In the case of the evaporation dataset, the subdataset is called **evap**.\n",
    "\n",
    "#### Shape File\n",
    "\n",
    "We obtained a shapefile containing block groups. We will be getting the evaporation data for each feature in the file.\n",
    "You can [download the shapefile from my Google Drive](https://drive.google.com/drive/folders/1cpv0hpLXwHxOlHlBpBeZek60SJ4why4y?usp=sharing).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Converting from NetCDF to GeoTIFF\n",
    "\n",
    "\n",
    "We must first convert the file from NetCDF to GeoTIFF format. GeoTIFF stores geospatial data in raster (image) form.\n",
    "\n",
    "We use the Geospatial Data Abstraction Library (GDAL) for this task. GDAL is commonly used in GIS applications, and many GIS Python programs build off of it.\n",
    "\n",
    "To convert the file, we need to define a desired projection. We use [EPSG 5070](https://epsg.io/5070-1252)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GDAL\n",
    "conda install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal  # import gdal\n",
    "\n",
    "path_to_file = './Desktop/evap.mon.mean.nc' # path to your evap.mon.mean file downloaded in step 1\n",
    "\n",
    "# We define the projection used in this project using its proj4 string (link above)\n",
    "proj = '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=1,1,-1,0,0,0,0 +units=m +no_defs'\n",
    "\n",
    "# We define custom warp options:\n",
    "    # -t_srs \"projection_string\" - defines the projection\n",
    "    # -of GTiff - defines the output format (GeoTIFF)\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "\n",
    "#Define the subdataset\n",
    "subdataset = 'evap'\n",
    "\n",
    "#Perform the warp\n",
    "    # Note that we prepend the NetCDF file with NETCDF: and end with the subdataset.\n",
    "        # Ex. NETCDF:C:/user/evap.mon.mean:evap\n",
    "ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + path_to_file + ':' + subdataset, destNameOrDestDS='./evap.mon.mean.geotiff', options=warp_options)  # Warp the NetCDF, and store the output in the output directory\n",
    "\n",
    "ds = None # This makes sure the cache is flushed and the file is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a multi-banded GeoTIFF file with the proper projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Extracting the bands (Optional)\n",
    "\n",
    "Now we have a multi-banded GeoTIFF file with the proper projection. To extract these bands, we will employ another function in GDAL, called gdal_translate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_geotiff = './evap.mon.mean.geotiff' # Define the path to your newly created geotiff file\n",
    "output_folder = './geotiffs/' # Define the path to a folder which will contain the output GeoTIFFs\n",
    "\n",
    "in_ds = gdal.Open(path_to_geotiff)  # Open the geotiff using GDAL\n",
    "band_count = in_ds.RasterCount # Get the total number of bands available (489 at the time of writing)\n",
    "\n",
    "year = 1979 # Start year\n",
    "month = 1 # Start month\n",
    "\n",
    "for i in range(1, band_count+1): # Loop through each band\n",
    "    filename = str(year) + '_' + str(month) # File name format: YYYY_MM\n",
    "    \n",
    "    if month < 10:  # Add a '0' to the name so the file lengths are consistent\n",
    "        filename = str(year) + '_0' + str(month)\n",
    "    \n",
    "    # Define the translation options\n",
    "        # -b : band number\n",
    "        # -of GTiff: output format (GeoTIFF)\n",
    "    translate_options = gdal.TranslateOptions(options='-b ' + str(i) + ' -of GTiff')\n",
    "    # Perform the translation\n",
    "    gdal.Translate(destName=output_folder + filename + '.geotiff', srcDS=in_ds, options=translate_options)\n",
    "    \n",
    "    # Increment the year and month\n",
    "    if i % 12 == 0:\n",
    "        year += 1\n",
    "    if month >= 12:\n",
    "        month = 0\n",
    "    month+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an individual GeoTIFF for each month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Calculating Average Evaporation (QGIS)\n",
    "\n",
    "We opted to initially use an average evaporation data across the 34 years in the SWMM simulation. To do this, we need to create GeoTIFFs that contain the average value for each pixel across each month for the 34 years in the data set.\n",
    "\n",
    "It is possible to run this code in standard Python, but we would need to modify our PATH variable with the QGIS program path. Opening QGIS is a simpler and faster solution.\n",
    "\n",
    "Please run the following code in QGIS.\n",
    "\n",
    "Please note that QGIS handles file directories based on the location of the **QGIS Project File**. Therefore, make sure the output directory is either absolute (ex. C:/users/path_to_file), or is located relative to your QGIS project file.\n",
    "\n",
    "#### Instructions for running code in QGIS:\n",
    "1. Open QGIS\n",
    "2. Press Plugins -> Python Console\n",
    "3. In the box that opens below, press the \"Show Editor\" button (notebook icon)\n",
    "4. In the new box that opens, add the following code and press the Run icon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our desired range of the dataset\n",
    "start_year = 1979\n",
    "start_month = 1\n",
    "end_year = 2014\n",
    "end_month = 12\n",
    "\n",
    "path_to_geotiff = './evap.mon.mean.geotiff'\n",
    "output_directory = './average_geotiffs/'  # Where to store the average geotiffs\n",
    "\n",
    "layer = QgsRasterLayer(path_to_geotiff, 'evap.mon.mean')  # Create a layer in QGIS from the GeoTIFF, and call it 'evap.mon.mean'\n",
    "bands = layer.bandCount()  # Get the number of bands in the GeoTIFF\n",
    "\n",
    "\n",
    "# Set up the months so we can fill them with their corresponding bands\n",
    "    # Ex. Bands 1, 13, 25, etc. will go to January\n",
    "jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "months = [dec, jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov]\n",
    "\n",
    "start_band = (start_year - 1979) * 12 + start_month\n",
    "end_band = (end_year - 1979) * 12 + end_month\n",
    "\n",
    "\n",
    "if bands <= end_band:  # Input validation to prevent looping past the total number of bands available\n",
    "    end_band = bands\n",
    "\n",
    "    \n",
    "# Loop through the bands and fill the month lists\n",
    "for i in range(start_band, end_band+1):  \n",
    "    months[i % 12].append(i)\n",
    "\n",
    "    \n",
    "# Loop through each month and create an expression for it\n",
    "    #Ex. (evap.mon.mean@1 + evap.mon.mean@13 + ... + evap.mon.mean@421) / 35 - for January\n",
    "for month in months:\n",
    "    print(month)\n",
    "    calc_expression = '('\n",
    "    for band in month[:-1]:  # Add all of the years data\n",
    "        calc_expression += '\"evap.mon.mean@' + str(band) + '\" + '\n",
    "    calc_expression += '\"evap.mon.mean@' + str(month[-1]) + '\")'\n",
    "    calc_expression += ' / ' + str(end_year - start_year + 1)  # Divide by the total number of years\n",
    "\n",
    "\n",
    "# Create a list of CalculatorEntry objects (a list of the bands we use in each calculation)\n",
    "    raster_bands = []\n",
    "    for band in month:\n",
    "        ras = QgsRasterCalculatorEntry()\n",
    "        ras.ref = 'evap.mon.mean@' + str(band)\n",
    "        ras.raster = layer\n",
    "        ras.bandNumber = band\n",
    "        raster_bands.append(ras)\n",
    "\n",
    "# Perform the calculation\n",
    "    calc = QgsRasterCalculator(calc_expression, output_directory + str(month[0]) + '.geotiff', 'GTIFF', layer.extent(), layer.width(), layer.height(), raster_bands)\n",
    "    calc.processCalculation()\n",
    "    \n",
    "# Add the created layers to the QGIS instance\n",
    "    outputLayer = QgsRasterLayer(output_directory + str(month[0]) + '.geotiff', str(month[0]))\n",
    "    QgsProject.instance().addMapLayer(outputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code in your QGIS instance, you will see the newly created average layers added. You can inspect the data using the \"Identify Features\" tool (Ctrl + Shift + I)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Opening the shape file\n",
    "\n",
    "Upon [downloading the shape file](https://drive.google.com/drive/folders/1cpv0hpLXwHxOlHlBpBeZek60SJ4why4y?usp=sharing), you can open it in QGIS with the average geotiff files.\n",
    "\n",
    "#### Steps for opening the shape file in QGIS:\n",
    "1. Click Layer -> Add Layer -> Vector Layer -> Browse for Source\n",
    "2. Navigate to the shape file you downloaded, and select \"SelectBG_all_land_BGID_final.shp\"\n",
    "3. Click OK, and the layer should be added and visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Masking the Average GeoTIFFs\n",
    "\n",
    "Unfortunately, some of the block groups extend into the water on the geotiff data, which will give us a higher-than-expected value when extracting this data. Therefore, we need to apply the land mask provided by NOAA. \n",
    "\n",
    "You can [download the land mask in NetCDF format here](ftp://ftp.cdc.noaa.gov/Datasets/NARR/time_invariant/land.nc).\n",
    "I have already reprojected the land mask and converted it to GeoTIFF. I recommend [downloading this version](https://drive.google.com/open?id=1az8dD5jEUVeNutcss-32BM2yXzt4cuGJ) from my Google drive. \n",
    "\n",
    "If you would like to do this yourself, you can download the NetCDF and project it / convert it in a similar fashion to Step 2 above.\n",
    "\n",
    "The land mask's data consists of an array of 1's and 0's. The 1's represent the land, and the 0's represent water. By multiplying our data by the land mask's data, we can remove all of the water data (since multiplying by zero gives zero).\n",
    "\n",
    "We will use GDAL's raster calculator to accomplish this (similar to the average geotiff extraction above).\n",
    "\n",
    "First, we import glob, a utility for selecting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GDAL's raster calculator is somewhat hidden, we need to add GDAL's installation path to our PATH variable. We use the sys module to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following line of code to easily see which Anaconda environment you are running this notebook in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will try to automatically set the location of your GDAL install. If this does not work, manually change the sys.path.insert(1, '...') to the location of your GDAL scripts folder. It will be near your sys.executable directory.\n",
    "\n",
    "For example, mine is in 'C:/Users/matas/Anaconda3/Lib/site-packages/GDAL-2.3.3-py3.7-win-amd64.egg-info/scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sys.executable\n",
    "path = path[:path.rfind('\\\\')]\n",
    "path += '\\\\Lib\\\\site-packages\\\\GDAL-2.3.3-py3.7-win-amd64.egg-info\\\\scripts'\n",
    "sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to gdal_calc, we need to select our average geotiffs using glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './average_geotiffs/'\n",
    "average_geotiffs = glob.glob(output_directory + '*.geotiff')\n",
    "\n",
    "\n",
    "if len(average_geotiffs) is 12:\n",
    "    print('Found all files successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop through each average file and apply the mask to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file = './land_mask_block_groups_5070.geotiff'\n",
    "output_directory = './masked_average_geotiffs/'\n",
    "for file in average_geotiffs:\n",
    "    name = file[file.rfind('\\\\')+1:file.rfind('.')]\n",
    "    # Parameter Description\n",
    "        # A*B - what calculation to perform\n",
    "        # A - the first raster\n",
    "        # B - the second raster\n",
    "        # outfile - the desired output location\n",
    "        # NoDataValue - this sets the output geotiff's NoData value to 0 (the land mask's water value)\n",
    "        # format - this sets the output file format (GTiff = GeoTIFF)\n",
    "    gdal_calc.Calc('A*B', A=file, B=mask_file, outfile= output_directory + name + '.geotiff', format='GTiff', NoDataValue=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get warnings that there was an overflow during the multiplication. This is because NARR's original NoData value is a very large number, and when we multiply, we experience an overflow.\n",
    "\n",
    "This does not effect the data as it is already a NoData value.\n",
    "\n",
    "Now, we have 12 masked, average geotiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Interpolating Data\n",
    "\n",
    "Since some of the block groups originally extended into the water, they now have no evaporation data in the new masked files, as shown in the image below\n",
    "\n",
    "<img src=\"https://i.imgur.com/daJsRUw.png\" width=\"300\">\n",
    "\n",
    "We will fix this by extending the raster by one pixel in each direction. We will get the data by taking an average of the data in the 3x3 square around the target pixel.\n",
    "\n",
    "This is the result of our interpolation:\n",
    "<img src=\"https://i.imgur.com/oV259nJ.png\" width=\"300\">\n",
    "\n",
    "Note that we can't see any block groups: This is good, it means they are all covered by the data.\n",
    "\n",
    "In order to modify raster datasets in Python, we use the rasterio module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation Function\n",
    "\n",
    "First, we need to define the function to take the average in a 3x3 square. It constructs a list of all possible (x,y) pairs and removes them from the list if it will go outside the index of the input array.\n",
    "\n",
    "It then loops through the remaining pairs and sums the data, then divides by the number of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_around_pixel(row, col, masked_array):\n",
    "    # This function will take an average value of the 3x3 square around a target pixel\n",
    "    # row, col provides the target coordinates\n",
    "    # masked_array is all of the data\n",
    "    \n",
    "    max_row = len(masked_array)\n",
    "    max_col = len(masked_array[0])\n",
    "    \n",
    "     # Define the coordinates of the square\n",
    "    top_left = (row-1, col-1)\n",
    "    top = (row - 1, col)\n",
    "    top_right = (row-1, col+1)\n",
    "\n",
    "    mid_left = (row, col-1)\n",
    "    mid_right = (row, col+1)\n",
    "\n",
    "    bottom_left = (row+1, col-1)\n",
    "    bottom = (row+1, col)\n",
    "    bottom_right = (row+1, col+1)\n",
    "    \n",
    "    # A list of all the possible pixels we will use in the average\n",
    "    pair_list = [top_left, top, top_right, mid_left, mid_right, bottom_left, bottom, bottom_right]\n",
    "\n",
    "    if row == 0: # We can't move up if the row is 0\n",
    "        if top_left in pair_list:\n",
    "            pair_list.remove(top_left)\n",
    "        if top in pair_list:\n",
    "            pair_list.remove(top)\n",
    "        if top_right in pair_list:\n",
    "            pair_list.remove(top_right)\n",
    "\n",
    "    if col == 0: # We can't move left if the col is 0\n",
    "        if top_left in pair_list:\n",
    "            pair_list.remove(top_left)\n",
    "        if mid_left in pair_list:\n",
    "            pair_list.remove(mid_left)\n",
    "        if bottom_left in pair_list:\n",
    "            pair_list.remove(bottom_left)\n",
    "\n",
    "    if row == max_row-1: # We can't move down if the row is max_row\n",
    "        if bottom_left in pair_list:\n",
    "            pair_list.remove(bottom_left)\n",
    "        if bottom in pair_list:\n",
    "            pair_list.remove(bottom)\n",
    "        if bottom_right in pair_list:\n",
    "            pair_list.remove(bottom_right)\n",
    "\n",
    "    if col == max_col - 1: # We can't move right if the col is max_col\n",
    "        if top_right in pair_list:\n",
    "            pair_list.remove(top_right)\n",
    "        if mid_right in pair_list:\n",
    "            pair_list.remove(mid_right)\n",
    "        if bottom_right in pair_list:\n",
    "            pair_list.remove(bottom_right)\n",
    "            \n",
    "    sum = 0\n",
    "    pair_count = 0\n",
    "    for pair in pair_list:\n",
    "        x = pair[0]\n",
    "        y = pair[1]\n",
    "        if not masked_array.mask[x][y]: # If the pair has a data value\n",
    "            sum += masked_array[x][y]\n",
    "            pair_count += 1\n",
    "        \n",
    "    if pair_count is 0:\n",
    "        return masked_array[x][y]\n",
    "    else:\n",
    "        return sum / pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './masked_average_geotiffs/'\n",
    "masked_files = glob.glob(output_directory + '*.geotiff')\n",
    "if len(masked_files) is 12:\n",
    "    print('Successfully found all files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the following code to use the function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './masked_average_extended_geotiffs/'\n",
    "\n",
    "for file in masked_files:\n",
    "    name = file[file.rfind('\\\\')+1:file.rfind('.')]\n",
    "\n",
    "    raster = rio.open(file) # Open the file with rasterio\n",
    "    data = raster.read(1) # Read band 1 of the raster (the only band)\n",
    "    # data is a numpy array\n",
    "    \n",
    "    rows = data.shape[0] # Get the number of rows using numpy's shape function\n",
    "    cols = data.shape[1] # columns\n",
    "    \n",
    "    nodata = raster.nodatavals # Get the nodata value of the raster\n",
    "\n",
    "    # The following line creates a numpy masked array. It allows us to determine if the target pixel has good data or not\n",
    "    # by creating a subarray of booleans, it will be True if the target pixel's data matches the nodata value.\n",
    "    data = np.ma.masked_equal(data, nodata)\n",
    "    output_data = np.copy(data) # Make a clean copy of the masked array because otherwise we will interpolate every pixel as we loop\n",
    "    \n",
    "    # Loop through each pixel in the raster\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if data.mask[i][j]:  # If there is no data\n",
    "                output_data[i][j] = square_around_pixel(i, j, data)  # Run the interpolate data function (below)\n",
    "                \n",
    "    with rio.open(output_directory + name + '.geotiff', 'w', **raster.profile) as dst:  # Open the output file\n",
    "        dst.write(output_data, 1)  # Write to band 1 of the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the averaged, masked, extended geotiff files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Obtaining the Evaporation Data for each Block Group\n",
    "Now, we can finally connect the shapefile and the geotiffs.\n",
    "We will use [rasterstats' zonal_stats function](https://pythonhosted.org/rasterstats/manual.html#zonal-statistics), which works by taking a weighted average of the data touched by each feature of the shapefile. Since there are many features in the shapefile, this is the most resource-intensive part of the process.\n",
    "\n",
    "We will use geopandas to read the shapefile as an array.\n",
    "\n",
    "First, we install rasterstats and geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rasterstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_directory = './masked_average_extended_geotiffs/'\n",
    "geotiffs = glob.glob(file_directory + '*.geotiff')\n",
    "if len(geotiffs) is 12:\n",
    "    print('Files found successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the shapefile into a dataframe using geopandas. Make sure all components of the shapefile are in the folder, not just the .shp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterstats as rs\n",
    "import pandas as pd\n",
    "\n",
    "path_to_shapefile = './shapefile/SelectBG_all_land_BGID_final.shp'\n",
    "shape_frame = gpd.read_file(path_to_shapefile)\n",
    "shape_frame['GEOID10'] = shape_frame['GEOID10'].astype(str)  # convert the GEOID10 column to str to preserve the leading zeroes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zonal_stats\n",
    "Now, we can loop through each geotiff and run zonal_stats against the shapefile.\n",
    "\n",
    "Note that we set the all_touched option to True. Since our block groups are so small compared to the raster pixels, this will use the entire pixel that covers them, instead of trying to find pixels within the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './evaporation_data/'\n",
    "\n",
    "output_statistics = ['count','min','max','mean']\n",
    "for file in geotiffs:\n",
    "    name = file[file.rfind('\\\\')+1:file.rfind('.')]\n",
    "    print('Started', name)\n",
    "    \n",
    "    stats = rs.zonal_stats(shape_frame, file, stats=output_statistics, all_touched=True)\n",
    "    frame = pd.DataFrame.from_dict(stats)\n",
    "    frame = frame.join(shape_frame['GEOID10'])\n",
    "    frame.to_csv(output_directory + name + '.csv')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading\n",
    "Instead of looping through each file, we can use multi-threading to speed the process up. This works by assigning a pool to the multithread_helper function. The pool grabs all the available threads and assigns them each one file. If you happen to have a >=12 core CPU, this means you can finish the zonal_stats processing in one iteration.\n",
    "\n",
    "This code may require some tinkering to work, I'm not sure if it works on Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def multithread_helper(file, shape_file):\n",
    "    name = file[file.rfind('\\\\')+1:file.rfind('.')]\n",
    "    print('Started', name)\n",
    "    return\n",
    "\n",
    "#     stats = rs.zonal_stats(shape_frame, file, stats=output_statistics, all_touched=True)\n",
    "#     frame = pd.DataFrame.from_dict(stats)\n",
    "#     frame = frame.join(shape_frame['GEOID10'])\n",
    "#     frame.to_csv(output_directory + name + '.csv')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    pool.map(partial(multithread_helper, shape_file = shape_frame), geotiffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Preparing the Data for Use\n",
    "\n",
    "#### Combining the CSV files\n",
    "\n",
    "This next step will change depending on how you plan to implement the data. Since we are using the data to create input files, I will combine all twelve .csv files into one pandas dataframe .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "csv_files = glob.glob('../data/input_file_data/evap_csv/*.csv')\n",
    "if len(csv_files) is 12:\n",
    "    print('Files found successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load one of the .csv files into the dataframe, and then use a join operation to combine the others into this one.\n",
    "We need to drop some of the columns that we won't be using, as well as 'Unnamed: 0', which is automatically created by pandas.\n",
    "\n",
    "We define the datatype of the GEOID10 column as a string so that the leading zeroes are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv(csv_files[0], dtype={'GEOID10':str})\n",
    "\n",
    "frame = frame.drop(['Unnamed: 0', 'min', 'max'], axis=1)\n",
    "frame = frame.set_index('GEOID10')\n",
    "\n",
    "# Change the column name from 'mean' to 'jan'\n",
    "temp = frame.columns.values.T.tolist()\n",
    "temp[0] = 'jan'\n",
    "frame.columns = temp\n",
    "\n",
    "frame = frame[['count', 'jan']]  # Reorder the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other dataframes, we need to convert the column names to their respective months, and then join onto the original frame. We join based on the 'GEOID10' column since this is the unique identifier in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['placeholder', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "for file in csv_files[1:]:\n",
    "    # Convert number to month\n",
    "    name = file[file.rfind('\\\\')+1:file.rfind('_')]\n",
    "    name = name[:2]\n",
    "    name = months[int(name)]\n",
    "    print(name)\n",
    "    \n",
    "    new_frame = pd.read_csv(file, dtype={'GEOID10':str})\n",
    "    new_frame = new_frame.drop(['Unnamed: 0', 'min', 'max', 'count'], axis=1)\n",
    "    \n",
    "    # Change the columns names\n",
    "    temp = new_frame.columns.values.T.tolist()\n",
    "    temp[0] = name\n",
    "    new_frame.columns = temp\n",
    "    \n",
    "    new_frame = new_frame.set_index('GEOID10')\n",
    "    \n",
    "#     print(frame)\n",
    "    frame = pd.merge(left=frame, right=new_frame, left_on='GEOID10', right_on='GEOID10')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a dataframe containing the evaporation data for each month. To save this, we will use pandas' to_pickle function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(frame, '../evaporation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SWMM Unit Conversion\n",
    "\n",
    "As we plan to use this data for SWMM, we need to make sure our units are consistent. Since we are using imperial units for the other datasets, we need to convert to inches per day.\n",
    "\n",
    "NARR's evaporation data is in millimeters. Since NARR accumulates data on a 3-hour basis, and then calculates monthly averages based on this, we need to multiply by 8 (8 * 3 = 24 hours) as well as convert from mm to inches \n",
    "\n",
    "inches / day (SWMM) = mm (NARR) * 8 * 0.0393701\n",
    "\n",
    "We will use pandas' apply function to apply this conversion to each column.\n",
    "\n",
    "Normally, we would need to define a custom function that accepts each column and performs an operation on it. Since the conversion is fairly simple, and only one line, we will use a lambda function to avoid defining a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_pickle('../data/input_file_data/evaporation.pkl')\n",
    "columns = frame.columns\n",
    "for column in columns[1:]:\n",
    "    frame[column] = frame[column].apply(lambda x: x * 8 * 0.0393701)\n",
    "\n",
    "frame.to_pickle('../data/input_file_data/evaporation_converted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now converted the evaporation data to inches per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have successfully extracted the evaporation data in pandas pickle format for each block group, starting from the NARR NetCDF.\n",
    "This can now be used in the SWMM processing by reading the dataframe and searching for the GEOID10 we need.\n",
    "\n",
    "**Contact:**\n",
    "\n",
    "Matas Lauzadis\n",
    "\n",
    "matas.lauzadis@gmail.com\n",
    "\n",
    "matasl2@illinois.edu\n",
    "\n",
    "[GitHub](https://www.github.com/mataslauzadis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
