{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WNV Model - NARR Precipitation Data Preprocessing\n",
    "In this notebook we will downlod the daily precipitation and air surface temperature using this [daily dataset from NARR](https://www.esrl.noaa.gov/psd/data/gridded/data.narr.html).\n",
    "\n",
    "We will then convert this data to county-level pandas dataframes using rasterstats zonal_stats function.\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wget  # Downloading Data\n",
    "except:\n",
    "    print('wget not found. Please run \\'pip install wget\\'.')\n",
    "\n",
    "try:\n",
    "    from osgeo import gdal  # GDAL\n",
    "except:\n",
    "    print('GDAL not found, please run \\'conda install -c conda-forge gdal\\'.')\n",
    "\n",
    "try: \n",
    "    import rasterio as rio  # Modifying Raster datasets\n",
    "except:\n",
    "    print('Rasterio not found, please run \\'pip install rasterio\\'.')\n",
    "    \n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except:\n",
    "    print('geopandas not found, please run \\'conda install geopandas\\'')\n",
    "    \n",
    "try: \n",
    "    import rasterstats as rs\n",
    "except:\n",
    "    print('rasterstats not found, please run \\'pip install rasterstats\\'')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import sys, os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Downloading the Precipitation Data\n",
    "\n",
    "First, we will download the data. We will use the [wget](https://pypi.org/project/wget/) module to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Download Directory\n",
    "Note that this is relative to the jupyter notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './yearly_netcdf_data/'\n",
    "if not os.path.exists(download_dir):\n",
    "    print('Directory not found, attempting to create it')\n",
    "    os.mkdir(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False  # Whether to replace existing files or not\n",
    "\n",
    "url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/Dailies/monolevel/'\n",
    "\n",
    "start_year = 1999\n",
    "end_year = 2015\n",
    "years = [start_year + i for i in range(end_year+1-start_year)]\n",
    "\n",
    "for year in years:\n",
    "    print(year)\n",
    "    file_name = 'apcp.' + str(year) + '.nc'\n",
    "\n",
    "    if os.path.exists(download_dir + file_name) and not overwrite:\n",
    "        pass\n",
    "    else:\n",
    "        download_url = url + 'apcp.' + str(year) + '.nc'\n",
    "        wget.download(download_url, download_dir + 'apcp.' + str(year) + '.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Converting the NetCDFs to GeoTiff\n",
    "\n",
    "We will convert each year's NetCDF file to a multi-banded GeoTIFF file, where each band represents one day of that year.\n",
    "\n",
    "This will require the use of GDAL.\n",
    "#### Selecting the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_netcdf = glob.glob(download_dir + '/*.nc')\n",
    "print('Successfully found files' if len(yearly_netcdf) == 17 else 'Failed to find files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Projection\n",
    "The WNV Model's county shapefile uses the EPSG 4326 projection, so we will use this for our GeoTIFF extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warping the NetCDFs to our Projection\n",
    "First, we will warp each NetCDF to our specified projection using the [gdalwarp utility](https://gdal.org/programs/gdalwarp.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './yearly_geotiff_data/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Directory not found, attempting to create it')\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False  # Overwrite existing GeoTIFFs or not\n",
    "\n",
    "\n",
    "# The warp options specify our target projection and output type (GeoTIFF)\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "subdataset = 'apcp'\n",
    "\n",
    "for file in yearly_netcdf:\n",
    "    name = file[-12:-3]\n",
    "    print('Processing:', name)\n",
    "    if os.path.exists(output_directory + name + '.geotiff') and not overwrite:\n",
    "        pass\n",
    "    else:\n",
    "        ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + file + ':' + subdataset, destNameOrDestDS=output_directory + name + '.geotiff', options=warp_options)\n",
    "        ds = None # Flush the file cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preparing the Land Mask\n",
    "Since some counties lie on the edge of the continent, their data mixes with the ocean data. Therefore, we need to apply the land mask provided by NARR on each year's GeoTIFF that we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./land.nc'):\n",
    "    print('File not found, attempting download.')\n",
    "    download_url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/time_invariant/land.nc'\n",
    "    wget.download(download_url, './land.nc')\n",
    "    \n",
    "mask_file = glob.glob('./land.nc')\n",
    "print('Successfully found land mask' if len(mask_file)==1 else 'Failed to find land mask')\n",
    "mask_file = mask_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting the Land Mask\n",
    "In order for the land mask's values to line up with our projected GeoTIFF files, we need to project it in a similar fashion to the GeoTIFF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = '+proj=longlat +datum=WGS84 +no_defs'\n",
    "subdataset = 'land'\n",
    "output_file = './land_mask_geotiff'\n",
    "\n",
    "warp_options = gdal.WarpOptions(options='-t_srs ' + proj + ' -of GTiff')\n",
    "ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + mask_file + ':' + subdataset, destNameOrDestDS=output_file + '.geotiff', options=warp_options)\n",
    "ds = None  # THIS IS VERY IMPORTANT\n",
    "print('Successfully Warped Land Mask (' + output_file + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Land Mask\n",
    "\n",
    "Since some of the counties lie outside the land mask's region, we need to extend the land mask by one pixel in each direction. This will ensure proper data coverage once we mask the geotiffs.\n",
    "\n",
    "#### Results of Extension Function\n",
    "<img src=\"https://i.imgur.com/dJqJeJS.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extension Function\n",
    "\n",
    "By creating a numpy masked array, we can create an array of booleans, which represent whether the pixel has data or does not have data. Then we can extend each of the pixels which have no data (if they have a neighbor with data).\n",
    "\n",
    "In this case, the data is a 1 or a 0, which represents land or not land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = rio.open('./land_mask_geotiff.geotiff')\n",
    "data = raster.read(1)\n",
    "\n",
    "rows = data.shape[0]\n",
    "cols = data.shape[1]\n",
    "nodata = 0\n",
    "\n",
    "data = np.ma.masked_equal(data,nodata)\n",
    "output_data = np.copy(data)\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if data.mask[i][j]:  # If there's no data\n",
    "            try:\n",
    "                for x in range(-1,2):\n",
    "                    for y in range(-1,2):\n",
    "                        if not data.mask[i+x][j+y]:\n",
    "                            output_data[i][j] = data[i+x][j+y]\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "with rio.open('./extended_land_mask.geotiff', 'w', **raster.profile) as dst:\n",
    "    dst.write(output_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Applying the Land Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining path to gdal_calc.py\n",
    "Please manually define your GDAL install path. If you're using Anaconda, it should be in your environment's bin folder.\n",
    "We will use this path to find gdal_calc.py, a script that is included with GDAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal_path = '/home/matas/anaconda3/envs/swmm/bin/'\n",
    "sys.path.insert(0, gdal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gdal_calc\n",
    "except:\n",
    "    print('gdal_calc not found, please specify the path to this file in the cell above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking the Yearly GeoTIFF Data\n",
    "\n",
    "We will take each year's GeoTIFF file and iterate over all of its bands to create 365 GeoTIFF images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_geotiff = glob.glob('./yearly_geotiff_data/*.geotiff')\n",
    "print('Successfully found all files' if len(yearly_geotiff) == 17 else 'Failed to find files')\n",
    "\n",
    "output_directory = './masked_extended_geotiffs/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "\n",
    "mask_file = './extended_land_mask.geotiff'\n",
    "if not mask_file:\n",
    "    print('Failed to find mask file!')\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    print('Found mask file.')\n",
    "\n",
    "for geotiff in yearly_geotiff:\n",
    "    \n",
    "    # Get number of bands to iterate over\n",
    "    raster = gdal.Open(geotiff)\n",
    "    band_count = raster.RasterCount\n",
    "    \n",
    "    # Specify the year\n",
    "    year = geotiff[geotiff.rfind('/')+6:geotiff.rfind('.')]\n",
    "    print(year)\n",
    "    year_dir = output_directory + year + '/'\n",
    "    if not os.path.exists(year_dir):\n",
    "        print('Making', year_dir)\n",
    "        os.makedirs(year_dir)\n",
    "    \n",
    "    # Loop through the year's 365 bands\n",
    "    for band in range(1, band_count+1):\n",
    "        outfile = year_dir + str(band) + '_masked_extended.geotiff'\n",
    "        if not os.path.exists(outfile) or overwrite:\n",
    "            print('Creating', outfile)\n",
    "            gdal_calc.Calc('A*B', A=geotiff, B=mask_file, A_band=band, outfile=outfile, format='GTiff', NoDataValue=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Extracting the Data\n",
    "\n",
    "We will use rasterstats' zonal_stats function to extract the data from each day into individual .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = './county_data/output_csv/'\n",
    "if not os.path.exists(output_directory):\n",
    "    print('Making', output_directory)\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Shape File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_frame = gpd.read_file('./urban_counties/urban_counties_wnv_4326.shp')\n",
    "shape = shape_frame.to_crs('+proj=longlat +datum=WGS84 +no_defs')\n",
    "shape['GEOID'] = shape['GEOID'].astype(str)  # Convert to string to keep the leading zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = ['count', 'mean']\n",
    "\n",
    "for year in range(1999, 2016):\n",
    "    year_dir = output_directory + str(year) + '/'\n",
    "    if not os.path.exists(year_dir):\n",
    "        print('Making', year_dir)\n",
    "        os.mkdir(year_dir)\n",
    "        \n",
    "        year_files = glob.glob('./masked_extended_geotiffs/' + str(year) + '/*.geotiff')\n",
    "        year_files = sorted(year_files)\n",
    "        \n",
    "        for file in year_files:\n",
    "            name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "            stats = rs.zonal_stats(shape, file, stats=output_columns, all_touched=True)\n",
    "            frame = pd.DataFrame.from_dict(stats)\n",
    "            \n",
    "            frame = frame.join(shape_frame['GEOID'])  # Add the GEOID's to the zonal_stats frame\n",
    "            frame = frame.join(shape_frame['NAME'])   \n",
    "            frame.to_csv(year_dir + name + '.csv')\n",
    "            \n",
    "            print(year_dir + name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Processing the Data\n",
    "\n",
    "We will load in each year's data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "for year in range(1999,2016):\n",
    "#     print(year)\n",
    "    \n",
    "    data_files = glob.glob('./county_data/output_csv/' + str(year) + '/*.csv')\n",
    "    if len(data_files) < 365 :\n",
    "        print('Failed to find files for', str(year), '!')\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "    frame = pd.read_csv(data_files[0])\n",
    "    frame.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    frame = frame[['NAME', 'GEOID', 'count', 'mean']]\n",
    "    \n",
    "    file = data_files[0]\n",
    "    name = file[file.rfind('/')+1:file.rfind('_')]\n",
    "    name = name[:name.rfind('_')]\n",
    "    cols = frame.columns.values\n",
    "    cols[3] = name\n",
    "    frame.columns = cols\n",
    "    \n",
    "    for file in data_files[1:]:\n",
    "        name = file[file.rfind('/')+1:file.rfind('_')]\n",
    "        name = name[:name.rfind('_')]\n",
    "        sub_frame = pd.read_csv(file)\n",
    "        sub_frame.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        \n",
    "        cols = sub_frame.columns.values\n",
    "        cols[0] = name\n",
    "        sub_frame.columns = cols\n",
    "        frame = frame.join(sub_frame[name])\n",
    "    \n",
    "    # Reorder the columns (1,2,3, ... ,364, 365)\n",
    "    data_columns = frame.iloc[:,3:]\n",
    "    data_columns.columns = data_columns.columns.astype(int)\n",
    "    data_columns = data_columns.sort_index(axis=1)\n",
    "    \n",
    "    # Convert data to inches\n",
    "    data_columns = data_columns.apply(lambda x: x / 25.40) \n",
    "    frame = frame.iloc[:,:2].join(data_columns)\n",
    "    \n",
    "    # Replace NaN values with 0 \n",
    "    frame.fillna(value=0, inplace=True)\n",
    "    \n",
    "    # Save the frame to disk\n",
    "    frame.to_pickle('./county_data/dataframes/all_data/' + str(year) + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Daily Air Temperature Data\n",
    "\n",
    "We will repeat the above process with the air temperature data for each county, so we can filter the snow precipitation in our data analysis.\n",
    "\n",
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './air_temperature/netcdfs/'\n",
    "if not os.path.exists(download_dir):\n",
    "    print('Making', download_dir)\n",
    "    os.makedirs(download_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "download_url = 'ftp://ftp.cdc.noaa.gov/Datasets/NARR/Dailies/monolevel/'\n",
    "dataset = 'air.sfc'\n",
    "for year in range(1999, 2016):\n",
    "    name = dataset + '.' + str(year) + '.nc'\n",
    "    url = download_url + name\n",
    "    if not os.path.exists(download_dir + name) or overwrite:\n",
    "        wget.download(url, download_dir + dataset + '.' + str(year) + '.nc')\n",
    "    else:\n",
    "        print('Skipping', name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and Project the NetCDF Data to GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "output_dir = './air_temperature/geotiffs/'\n",
    "proj = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs'\n",
    "warp_options = gdal.WarpOptions(options='-t_srs \\\"' + proj + '\\\" -of GTiff')\n",
    "subdataset = 'air'\n",
    "\n",
    "\n",
    "files = glob.glob('./air_temperature/netcdfs/*.nc')\n",
    "print('Found files' if len(files) == 17 else 'Some files may be missing!')\n",
    "\n",
    "for file in sorted(files):\n",
    "    name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    if not os.path.exists(output_dir + name) or overwrite:\n",
    "        print(name)\n",
    "        ds = gdal.Warp(srcDSOrSrcDSTab='NETCDF:' + file + ':' + subdataset, destNameOrDestDS=output_dir + name + '.geotiff', options=warp_options)\n",
    "        ds = None # Flush the file cache\n",
    "    else:\n",
    "        print('Skipping', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask and Extract Each Band of the GeoTIFF\n",
    "This is assuming you have already created **and extended** the land mask in step 3.\n",
    "\n",
    "#### Import Gdal_Calc\n",
    "Please manually define your GDAL install path. If you're using Anaconda, it should be in your environment's bin folder.\n",
    "We will use this path to find gdal_calc.py, a script that is included with GDAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal_path = '/home/matas/anaconda3/envs/swmm/bin/'\n",
    "sys.path.insert(0, gdal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gdal_calc\n",
    "except:\n",
    "    print('gdal_calc not found, please specify the path to this file in the cell above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "output_dir = './air_temperature/daily_geotiffs/'\n",
    "\n",
    "# Select the Projected GeoTIFFs\n",
    "files = glob.glob('./air_temperature/geotiffs/*.geotiff')\n",
    "print('Found all files' if len(files) == 17 else ('Failed to find files!'))\n",
    "\n",
    "# Select the Mask File\n",
    "mask_file = glob.glob('./extended_land_mask.geotiff')\n",
    "if len(mask_file) == 1:\n",
    "    mask_file = mask_file[0]\n",
    "    print('Found mask file!')\n",
    "else:\n",
    "    print('Failed to find mask file.')\n",
    "    raise KeyboardInterrupt\n",
    "    \n",
    "\n",
    "for file in sorted(files):\n",
    "    name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    year = name[name.rfind('.')+1:]\n",
    "    \n",
    "    year_dir = output_dir + str(year) + '/'\n",
    "    if not os.path.exists(year_dir):\n",
    "        print('Making', year_dir)\n",
    "        os.makedirs(year_dir)\n",
    "    \n",
    "    # Get number of bands to iterate over\n",
    "    raster = gdal.Open(file)\n",
    "    band_count = raster.RasterCount\n",
    " \n",
    "    # Loop through the year's 365 bands\n",
    "    for band in range(1, band_count+1):\n",
    "        outfile = year_dir + str(band) + '.geotiff'\n",
    "        if not os.path.exists(outfile) or overwrite:\n",
    "            print('Creating', outfile)\n",
    "            gdal_calc.Calc('A*B', A=file, B=mask_file, A_band=band, outfile=outfile, format='GTiff', NoDataValue=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal Stats\n",
    "\n",
    "We will now extract the GeoTIFF data for each county, day, year.\n",
    "\n",
    "The following code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterstats as rs\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print('Started')\n",
    "shape_frame = gpd.read_file('./urban_counties/urban_counties_wnv_4326.gpkg')\n",
    "shape_frame['GEOID'] = shape_frame['GEOID'].astype(str)\n",
    "\n",
    "print(shape_frame.crs)\n",
    "output = ['mean', 'count']\n",
    "\n",
    "\n",
    "for year in range(1999,2016):\n",
    "    year_geotiffs = glob.glob('./air_temperature/daily_geotiffs/' + str(year) + '/*.geotiff')\n",
    "\n",
    "    year_dir = './air_temperature/output/' + str(year) + '/' \n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "    for file in year_geotiffs:\n",
    "        name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "        print(year, '-', name)\n",
    "        stats = rs.zonal_stats(shape_frame, file, stats=output, all_touched=Tru$\n",
    "        frame = pd.DataFrame.from_dict(stats)\n",
    "        frame = frame.join(shape_frame['GEOID'])\n",
    "        frame = frame.join(shape_frame['NAME'])\n",
    "        frame.to_csv(year_dir + name + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the CSVs to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "out_dir = './air_temperature/dataframes/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for year in range(1999, 2016):\n",
    "    files = glob.glob('./air_temperature/output/' + str(year) + '/*.csv')\n",
    "    \n",
    "    # Create the dataframe with the first file\n",
    "    frame = pd.read_csv(files[0])\n",
    "    frame = frame.drop(['Unnamed: 0', 'count'], axis=1)\n",
    "    frame = frame.set_index('GEOID', drop=True)\n",
    "    frame = frame[['NAME', 'mean']]\n",
    "    \n",
    "    # Change 'mean' to day number\n",
    "    day = files[0][files[0].rfind('/')+1:files[0].rfind('.')]\n",
    "    cols = frame.columns.values\n",
    "    cols[1] = day\n",
    "    frame.columns = cols\n",
    "    \n",
    "    for file in files[1:]:\n",
    "        day = file[file.rfind('/')+1:file.rfind('.')]\n",
    "        \n",
    "        sub_frame = pd.read_csv(file)\n",
    "        sub_frame = sub_frame.set_index('GEOID', drop=True)\n",
    "        \n",
    "        cols = sub_frame.columns.values\n",
    "        cols[1] = day\n",
    "        \n",
    "        frame = frame.join(sub_frame[day])\n",
    "       \n",
    "    \n",
    "    data_columns = frame.iloc[:,1:]\n",
    "    data_columns.columns = data_columns.columns.astype(int)\n",
    "    data_columns = data_columns.sort_index(axis=1)\n",
    "    \n",
    "    \n",
    "    frame = frame.iloc[:,:1].join(data_columns)\n",
    "    \n",
    "    if not os.path.exists(out_dir + str(year) + '.pkl') or overwrite:\n",
    "        frame.to_pickle(out_dir + str(year) + '.pkl')\n",
    "        print('Writing', str(year) + '.pkl')\n",
    "    else:\n",
    "        print('Skipping', str(year) + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Selecting the Data from June 1st to August 1\n",
    "This will be our summer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './county_data/dataframes/summer_data/'\n",
    "if not os.path.exists(output_dir):\n",
    "    print('Making', output_dir)\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "\n",
    "for year in range(1999,2016):\n",
    "    frame = pd.read_pickle('./processed_output/' + str(year) + '.pkl')\n",
    "    \n",
    "    # LEAP YEAR: Our data is offset by one day\n",
    "    if len(frame.columns) == 368:\n",
    "        # Select Jun 1 to Aug 1\n",
    "        # Day of Year Calendar: https://nsidc.org/data/tools/doy_calendar.html\n",
    "        data = frame.iloc[:,154:216]\n",
    "    else:\n",
    "        data = frame.iloc[:,153:215]\n",
    "        \n",
    "    frame = frame.iloc[:,:2].join(data)\n",
    "    frame.set_index('GEOID', drop=True, inplace=True)\n",
    "    \n",
    "    if not os.path.exists(output_dir + str(year) + '.pkl') or overwrite:\n",
    "        print('Making', output_dir + str(year) + '.pkl')\n",
    "        frame.to_pickle(output_dir + str(year) + '.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Filtering the Data by Temperature\n",
    "\n",
    "Since we will now consider the entire year's precipitation, we need to filter out the data for each year where the air temperature is less than 0C, as the precipitation during this period will be snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "out_dir = './county_data/dataframes/filtered_freezing_data/'\n",
    "if not os.path.exists(out_dir):\n",
    "    print('Making', out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "for year in range(1999,2016):\n",
    "    rainfall_data = pd.read_pickle('./county_data/dataframes/all_data/' + str(year) + '.pkl').set_index('GEOID', drop=True)\n",
    "    rainfall = rainfall_data.iloc[:,1:]\n",
    "    temperature = pd.read_pickle('./air_temperature/dataframes/' + str(year) + '.pkl').iloc[:,1:]\n",
    "    \n",
    "    freezing = temperature > 273.15  # False if Temperature < 0C, True if > 0C\n",
    "    \n",
    "    freezing = freezing.applymap(lambda x: -1 if x is False else 1)  # convert false to -1, true to 1\n",
    "    \n",
    "    data = rainfall * freezing\n",
    "    data = data.applymap(lambda x: np.nan if x < 0 else x)  # convert freezing temperatures to NaN\n",
    "    data = data.applymap(lambda x: abs(x))  # to turn the -0.00 values to 0.00\n",
    "    \n",
    "    \n",
    "    frame = rainfall_data.iloc[:,0].to_frame().join(data)\n",
    "    \n",
    "    if not os.path.exists('./county_data/dataframes/filtered_freezing_data/' + str(year) + '.pkl') or overwrite:\n",
    "        frame.to_pickle('./county_data/dataframes/filtered_freezing_data/' + str(year) + '.pkl')\n",
    "    else:\n",
    "        print('Skipping', year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
